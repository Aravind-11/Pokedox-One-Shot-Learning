{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('tf')\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE DIMENSION\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img=[]\n",
    "img_data_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arvt/pokedox\n",
      "/users/arvt/pokedox/data/63.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected Ptr<cv::UMat> for argument 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-21a23ceb40b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#input_img_resize = cv2.resize(n, (128, 128))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected Ptr<cv::UMat> for argument 'img'"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "print(PATH)\n",
    "data_path = PATH + '/aug_data_2'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "num_channel = 1\n",
    "\n",
    "img_data_list = []\n",
    "i=0\n",
    "for dataset in data_dir_list:\n",
    "        #print(i)\n",
    "        #g=str(i)\n",
    "        #c=os.path.join(\"/users/arvt/pokedox/data/\",g)\n",
    "        #print(c)\n",
    "        #os.mkdir(c)\n",
    "        #print(dataset)\n",
    "        input_img = cv2.imread(data_path + '/' + dataset)\n",
    "        #input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        b=os.path.join('/users/arvt/pokedox/data/',dataset)\n",
    "        #os.mkdir(b)\n",
    "        print(b)\n",
    "        n=augment(input_img)\n",
    "        #print(n)\n",
    "        #input_img_resize = cv2.resize(n, (128, 128))\n",
    "        cv2.imwrite(b,n)\n",
    "        i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arvt/pokedox\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "print(PATH)\n",
    "data_path = PATH + '/aug_data_6'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "num_channel = 1\n",
    "img_data_list = []\n",
    "i=0\n",
    "for dataset in data_dir_list:\n",
    "        #print(i)\n",
    "        if(dataset=='.ipynb_checkpoints'):\n",
    "            continue\n",
    "        c=os.path.join(\"/users/arvt/pokedox/dataset17/\",dataset[:len(dataset)-4])\n",
    "        input_img = cv2.imread(data_path + '/' + dataset)\n",
    "        #print(input_img)\n",
    "        b=os.path.join(c,dataset+'4'+'.jpg')\n",
    "        #os.mkdir(b)\n",
    "        cv2.imwrite(b,input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 819\n",
    "\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,), dtype='int64')\n",
    "for i in range(len(labels)):\n",
    "    labels[i]=i\n",
    "\n",
    "names=[]\n",
    "for i in range(819):\n",
    "    names.append(i)\n",
    "Y = np_utils.to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = img_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = shuffle(img_data, Y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val,y_val=shuffle(img_data,Y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image):\n",
    "    \n",
    "    image = tf.image.resize_with_crop_or_pad(image,128 + 6, 128 + 6) \n",
    "    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    image=tf.image.central_crop(image, central_fraction=0.5)\n",
    "    image=tf.image.random_brightness(image, 0.5, seed=None)\n",
    "    #image=tf.image.random_contrast(image,0.1,0.7)\n",
    "    image=tf.image.random_saturation(image,0.1,0.7)\n",
    "    image=tf.image.random_hue(image,0.5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_augment(image,label):\n",
    "    \n",
    "    image = tf.image.resize_with_crop_or_pad(image,128 + 6, 128 + 6) \n",
    "    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    image=tf.image.central_crop(image, central_fraction=0.5)\n",
    "    image=tf.image.random_brightness(image, 0.5, seed=None)\n",
    "    #image=tf.image.random_contrast(image,0.1,0.7)\n",
    "    image=tf.image.random_saturation(image,0.1,0.7)\n",
    "    image=tf.image.random_hue(image,0.5)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=augment(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val,y_val=val_augment(x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15e6a8b80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the weights\n",
    "model.load_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    \n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        \n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            # read all the images in the current category\n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image,w,h):\n",
    "    image=cv2.resize(image,(w,h))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimgs(path,n = 0):\n",
    "    '''\n",
    "    path => Path of train directory or test directory\n",
    "    '''\n",
    "    X=[]\n",
    "    y = []\n",
    "    cat_dict = {}\n",
    "    lang_dict = {}\n",
    "    curr_y = n\n",
    "    \n",
    "    # we load every alphabet seperately so we can isolate them later\n",
    "    for alphabet in os.listdir(path):\n",
    "        print(\"loading alphabet: \" + alphabet)\n",
    "        lang_dict[alphabet] = [curr_y,None]\n",
    "        alphabet_path = os.path.join(path,alphabet)\n",
    "        #cat_dict[curr_y] = (alphabet, alphabet)\n",
    "        category_images=[]\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            image_path = os.path.join(alphabet_path, letter)\n",
    "            if(letter=='.ipynb_checkpoints'):\n",
    "                continue\n",
    "            #print(image_path)\n",
    "            image = cv2.imread(image_path)\n",
    "            #print(image)\n",
    "            image=resize_image(image,105,105)\n",
    "            image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            category_images.append(image)\n",
    "            y.append(curr_y)\n",
    "        try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "        except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "        curr_y += 1\n",
    "        lang_dict[alphabet][1] = curr_y - 1      \n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict\n",
    "    '''\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            print(letter)\n",
    "            \n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            print(letter_path)\n",
    "            # read all the images in the current category\n",
    "            image = cv2.imread(letter_path)\n",
    "            category_images.append(image)\n",
    "            y.append(curr_y)\n",
    "        try:\n",
    "            X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1    \n",
    "            \n",
    "            \n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "        \n",
    "        # every letter/category has it's own column in the array, so  load seperately\n",
    "        for letter in os.listdir(alphabet_path):\n",
    "            cat_dict[curr_y] = (alphabet, letter)\n",
    "            category_images=[]\n",
    "            print(letter)\n",
    "            \n",
    "            letter_path = os.path.join(alphabet_path, letter)\n",
    "            print(letter_path)\n",
    "            # read all the images in the current category\n",
    "            image = cv2.imread(letter_path)\n",
    "            category_images.append(image)\n",
    "            y.append(curr_y)\n",
    "        try:\n",
    "            X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1    \n",
    "            \n",
    "            \n",
    "            for filename in os.listdir(letter_path):\n",
    "                image_path = os.path.join(letter_path, filename)\n",
    "                image = imread(image_path)\n",
    "                category_images.append(image)\n",
    "                y.append(curr_y)\n",
    "            try:\n",
    "                X.append(np.stack(category_images))\n",
    "            # edge case  - last one\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                print(\"error - category_images:\", category_images)\n",
    "            curr_y += 1\n",
    "            lang_dict[alphabet][1] = curr_y - 1\n",
    "    y = np.vstack(y)\n",
    "    X = np.stack(X)\n",
    "    return X,y,lang_dict\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval,yval,cval=loadimgs('/users/arvt/pokedox/test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"val.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((Xval,cval),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 6, 40, 40)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,c = loadimgs('/users/arvt/pokedox/main data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('/users/arvt/pokedox/test data/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/users/arvt/pokedox/data'\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path,\"train.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((X,c),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir('/users/arvt/pokedox/main data/'):\n",
    "    \n",
    "    if(i=='.DS_Store'):print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,s=\"train\"):\n",
    "    \"\"\"\n",
    "    Create batch of n pairs, half same class, half different class\n",
    "    \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    #print(X.shape)\n",
    "    n_classes, n_examples, w, h= X.shape\n",
    "    \n",
    "    # randomly sample several classes to use in the batch\n",
    "    categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "    \n",
    "    # initialize 2 empty arrays for the input image batch\n",
    "    pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets=np.zeros((batch_size,))\n",
    "    \n",
    "    # make one half of it '1's, so 2nd half of batch has same class\n",
    "    targets[batch_size//2:] = 1\n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = rng.randint(0, n_examples)\n",
    "        pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "        idx_2 = rng.randint(0, n_examples)\n",
    "        \n",
    "        # pick images of same class for 1st half, different for 2nd\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            # add a random number to the category modulo n classes to ensure 2nd image has a different category\n",
    "            category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "        \n",
    "        pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    \"\"\"\n",
    "    a generator for batches, so model.fit_generator can be used.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size,s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.misc import imread\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 4096)         38947648    input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4096)         0           sequential_4[0][0]               \n",
      "                                                                 sequential_4[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            4097        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 38,951,745\n",
      "Trainable params: 38,951,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((105, 105, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training alphabets: \n",
      "\n",
      "['135', '307', '61', '95', '338', '300', '132', '59', '92', '66', '336', '104', '323-mega', '309', '50', '68', '103', '331', '57', '168', '157', '150', '159', '32', '25-libre', '166', '192', '319-mega', '35', '195', '161', '330', '102', '69', '56', '105', '337', '51', '308', '58', '133', '301', '67', '93', '306', '134', '94', '339', '60', '34', '303-mega', '160', '194', '33', '158', '193', '167', '151', '169', '156', '216', '229', '211', '227', '218', '302-mega', '220', '181-mega', '280', '274', '115-mega', '273', '142-mega', '287', '245', '289', '242', '221', '94-mega', '226', '219', '210', '217', '228', '288', '243', '244', '286', '272', '334-mega', '275', '281', '257', '268', '150-mega-x', '25-phd', '250', '292', '266', '259', '261', '295', '235', '232', '204', '203', '294', '260', '267', '293', '258', '260-mega', '251', '256', '150-mega-y', '269', '202', '205', '233', '234', '174', '180', '20', '187', '18', '173', '27', '9', '145', '11', '142', '7', '29', '308-mega', '16', '189', '129', '42', '89', '116', '324', '45', '6-mega-x', '323', '111', '73', '118', '87', '315', '127', '80', '74', '120', '312', '6', '28', '143', '257-mega', '17', '188', '201-f', '144', '1', '10', '340', '172', '186', '19', '26', '8', '181', '175', '21', '75', '81', '313', '121', '119', '86', '282-mega', '72', '126', '314', '6-mega-y', '44', '110', '322', '43', '128', '88', '325', '117', '254-mega', '198', '153', '38', '154', '36', '196', '162', '31', '165', '191', '131', '303', '91', '65', '304', '136', '62', '96', '109', '332', '100', '54', '107', '335', '98', '138', '53', '30', '190', '164', '37', '163', '197', '3-mega', '130-mega', '155', '199', '39', '152', '334', '106', '99', '52', '139', '101', '333', '55', '137', '305', '97', '108', '63', '302', '130', '64', '90', '270', '284', '248', '283', '277', '241', '279', '246', '212', '215', '223', '127-mega', '224', '278', '247', '240', '249', '276', '282', '285', '271', '225', '306-mega', '222', '65-mega', '214-mega', '214', '213', '310-mega', '231', '209', '236', '200', '238', '248-mega', '207', '253', '298', '254', '15-mega', '262', '296', '291', '265', '239', '206', '208', '9-mega', '237', '230', '264', '290', '297', '263', '25-belle', '255', '252', '229-mega', '299', '46', '79', '112', '320', '318', '41', '327', '115', '83', '18-mega', '77', '311', '123', '48', '70', '329', '84', '124', '316', '184', '170', '24', '177', '183', '148', '212-mega', '23', '141', '4', '25-pop-star', '15', '3', '146', '12', '179', '25-rock-star', '328', '85', '71', '317', '125', '76', '82', '49', '122', '310', '40', '319', '114', '326', '47', '321', '113', '78', '147', '2', '178', '13', '5', '140', '14', '80-mega', '182', '208-mega', '176', '22', '149', '171', '185', '25']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_path, \"train.pickle\"), \"rb\") as f:\n",
    "    (Xtrain, train_classes) = pickle.load(f)\n",
    "    \n",
    "print(\"Training alphabets: \\n\")\n",
    "print(list(train_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation alphabets:\n",
      "\n",
      "['362-mega', '551', '668f', '556', '569', '560', '594', '593', '567', '558', '391', '533', '645-therian', '676-diamond', '701', '365', '362', '706', '534', '396', '502', '354', '479-frost', '398', '708', '666-elegant', '353', '460-mega', '505', '566', '592', '381-mega', '559', '595', '561', '557', '568', '709', '399', '352', '504', '354-mega', '503', '355', '676-star', '585-winter', '397', '363', '535', '707', '413-sandy', '700', '532', '364', '390', '686', '672', '440', '447', '675', '681', '478', '471', '643', '485', '688', '482', '644', '476', '449', '610', '628', '648-pirouette', '617', '425', '621', '380-mega', '619', '414', '626', '477', '592f', '483', '448', '484', '645-incarnate', '470', '689', '680', '674', '446', '359-mega', '479', '441', '673', '687', '492-sky', '487-altered', '618', '627', '415', '620', '629', '424', '616', '611', '401', '633', '383-primal', '634', '406', '439', '602', '430', '437', '605', '408', '412-sandy', '651', '463', '497', '669', '490', '464', '656', '499', '694', '452', '660', '421-overcast', '658', '667', '455', '693', '604', '436', '409', '431', '603', '479-heat', '407', '635', '438', '632', '400', '585-summer', '659', '692', '681-shield', '454', '666', '498', '661', '453', '695', '668', '642-incarnate', '657', '465', '493-normal', '491', '386-attack', '496', '462', '650', '346', '510', '379', '517', '341', '719-mega', '647-resolute', '528', '383', '377', '713', '521', '348', '526', '714', '370', '384', '519', '572', '373-mega', '581', '575', '588', '543', '586-autumn', '544', '385', '715', '527', '371', '647-ordinary', '518', '376', '520', '712', '382', '349', '516', '529', '487-origin', '347', '511', '585-spring', '378', '545', '589', '542', '574', '580', '587', '573', '508', '361', '537', '705', '479-wash', '395', '359', '392', '702', '530', '366', '422-west', '539', '350', '506', '368', '501', '357', '552', '599', '590', '564', '563', '597', '369', '641-incarnate', '500', '356', '538', '351', '507', '358', '531', '642-therian', '703', '367', '393', '509', '394', '360', '704', '536', '596', '562', '386-speed', '565', '591', '553', '448-mega', '598', '554', '419', '426', '614', '613', '428', '625', '417', '410', '622', '676', '444', '682', '649', '685', '443', '671', '488', '481', '475', '678', '640', '472', '486', '413-trash', '586-winter', '412-plant', '623', '384-mega', '411', '429', '676-heart', '445-mega', '416', '624', '420', '612', '418', '615', '427', '473', '646', '474', '428-mega', '480', '423-west', '679', '670', '442', '684', '489', '683', '445', '677', '648', '458', '593f', '655', '467', '641-therian', '699', '460', '652', '585-autumn', '646-black', '494', '469', '456', '664', '690', '678f', '531-mega', '550-blue-striped', '697', '663', '451', '405', '637', '376-mega', '608', '630', '402', '606', '434', '639', '433', '601', '422-east', '450', '662', '696', '468', '691', '665', '457', '698', '681-blade', '495', '653', '461', '479-fan', '459', '466', '654', '600', '432', '382-primal', '492-land', '586-spring', '435', '607', '638', '403', '631', '550-red-striped', '636', '404', '421-sunshine', '609', '549', '521f', '582', '576', '386-defense', '571', '578', '547', '720-unbound', '413-plant', '540', '514', '342', '389', '719', '345', '721', '513', '717', '525', '373', '387', '380', '374', '522', '710', '423-east', '541', '579', '546', '584', '570', '548', '577', '583', '375', '711', '523', '381', '412-trash', '475-mega', '555-standard', '586-summer', '524', '716', '372', '479-mow', '386-normal', '344', '512', '646-white', '720', '515', '343', '718', '388']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(save_path, \"val.pickle\"), \"rb\") as f:\n",
    "    (Xval, val_classes) = pickle.load(f)\n",
    "\n",
    "print(\"Validation alphabets:\", end=\"\\n\\n\")\n",
    "print(list(val_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\", language=None):\n",
    "    \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "    if s == 'train':\n",
    "        X = Xtrain\n",
    "        categories = train_classes\n",
    "    else:\n",
    "        X = Xval\n",
    "        categories = val_classes\n",
    "    n_classes, n_examples, w, h = X.shape\n",
    "    \n",
    "    indices = rng.randint(0, n_examples,size=(N,))\n",
    "    if language is not None: # if language is specified, select characters for that language\n",
    "        low, high = categories[language]\n",
    "        if N > high - low:\n",
    "            raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "        categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "\n",
    "    else: # if no language specified just pick a bunch of random letters\n",
    "        categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "    true_category = categories[0]\n",
    "    ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "    test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "    support_set = X[categories,indices,:,:]\n",
    "    support_set[0,:,:] = X[true_category,ex2]\n",
    "    support_set = support_set.reshape(N, w, h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "    pairs = [test_image,support_set]\n",
    "\n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "evaluate_every = 200 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 20000 # No. of training iterations\n",
    "N_way =  6# how many classes for testing one-shot tasks\n",
    "n_val = 250 # how many one-shot tasks to validate on\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 200 iterations: 11.053556064764658 mins\n",
      "Train Loss: 0.7356560230255127\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.0% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 92.0, previous best: -1\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 400 iterations: 24.125943815708162 mins\n",
      "Train Loss: 0.7994027137756348\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.6% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 93.6, previous best: 92.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 600 iterations: 37.30372848113378 mins\n",
      "Train Loss: 0.548590362071991\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.6% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 93.6, previous best: 93.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 800 iterations: 49.18784114917119 mins\n",
      "Train Loss: 0.5538426637649536\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1000 iterations: 65.51080861488978 mins\n",
      "Train Loss: 0.5439085960388184\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1200 iterations: 79.01161493460337 mins\n",
      "Train Loss: 0.4480428397655487\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 95.6, previous best: 93.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1400 iterations: 92.916949848334 mins\n",
      "Train Loss: 0.3646053075790405\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1600 iterations: 107.40144766966502 mins\n",
      "Train Loss: 0.29602110385894775\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 98.0, previous best: 95.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 1800 iterations: 121.62386580308278 mins\n",
      "Train Loss: 0.38662654161453247\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2000 iterations: 136.80529205004373 mins\n",
      "Train Loss: 0.28417113423347473\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2200 iterations: 196.33313691616058 mins\n",
      "Train Loss: 0.20504185557365417\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2400 iterations: 257.8342159668604 mins\n",
      "Train Loss: 0.26800817251205444\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2600 iterations: 272.10015189647675 mins\n",
      "Train Loss: 0.19738003611564636\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 2800 iterations: 287.1862000823021 mins\n",
      "Train Loss: 0.25345543026924133\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3000 iterations: 300.1804441809654 mins\n",
      "Train Loss: 0.1641526222229004\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3200 iterations: 313.70881589651106 mins\n",
      "Train Loss: 0.1542268693447113\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3400 iterations: 328.21180763244627 mins\n",
      "Train Loss: 0.15466000139713287\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3600 iterations: 342.46547759771346 mins\n",
      "Train Loss: 0.14706414937973022\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 98.0, previous best: 98.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 3800 iterations: 357.7354915857315 mins\n",
      "Train Loss: 0.17214471101760864\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4000 iterations: 373.8463257630666 mins\n",
      "Train Loss: 0.13869613409042358\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4200 iterations: 390.7786353826523 mins\n",
      "Train Loss: 0.17176634073257446\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4400 iterations: 406.02939233382546 mins\n",
      "Train Loss: 0.13130663335323334\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 98.0, previous best: 98.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4600 iterations: 422.05795739889146 mins\n",
      "Train Loss: 0.18184751272201538\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 4800 iterations: 437.1378952503204 mins\n",
      "Train Loss: 0.22248972952365875\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 98.8, previous best: 98.0\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5000 iterations: 454.2426042675972 mins\n",
      "Train Loss: 0.19778703153133392\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5200 iterations: 471.18059528271357 mins\n",
      "Train Loss: 0.11153580248355865\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5400 iterations: 484.66907351414363 mins\n",
      "Train Loss: 0.1198526918888092\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5600 iterations: 500.2359474182129 mins\n",
      "Train Loss: 0.11747823655605316\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 5800 iterations: 515.9666373689969 mins\n",
      "Train Loss: 0.08706112951040268\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6000 iterations: 532.424409365654 mins\n",
      "Train Loss: 0.11027771234512329\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6200 iterations: 544.2513488491376 mins\n",
      "Train Loss: 0.18967776000499725\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6400 iterations: 556.934941248099 mins\n",
      "Train Loss: 0.10334285348653793\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 98.8, previous best: 98.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6600 iterations: 569.7886809984843 mins\n",
      "Train Loss: 0.11414290964603424\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 6800 iterations: 583.1006643851598 mins\n",
      "Train Loss: 0.14401313662528992\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7000 iterations: 592.0229175011317 mins\n",
      "Train Loss: 0.1722610890865326\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7200 iterations: 600.688549097379 mins\n",
      "Train Loss: 0.109551340341568\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.6% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 99.6, previous best: 98.8\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7400 iterations: 609.208721101284 mins\n",
      "Train Loss: 0.13832995295524597\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7600 iterations: 617.7475963155429 mins\n",
      "Train Loss: 0.13883258402347565\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 7800 iterations: 626.2299701333046 mins\n",
      "Train Loss: 0.11385206878185272\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8000 iterations: 635.5261465191841 mins\n",
      "Train Loss: 0.12904749810695648\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8200 iterations: 644.8319779157639 mins\n",
      "Train Loss: 0.1031813845038414\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8400 iterations: 654.0932227691014 mins\n",
      "Train Loss: 0.1250462830066681\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8600 iterations: 663.3346816182136 mins\n",
      "Train Loss: 0.11491107195615768\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 8800 iterations: 672.5796037356059 mins\n",
      "Train Loss: 0.17845748364925385\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9000 iterations: 681.8461232980093 mins\n",
      "Train Loss: 0.13151784241199493\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9200 iterations: 1114.1635679483413 mins\n",
      "Train Loss: 0.10272832214832306\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.6% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 99.6, previous best: 99.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9400 iterations: 1122.6142774502437 mins\n",
      "Train Loss: 0.10908355563879013\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9600 iterations: 1131.661345434189 mins\n",
      "Train Loss: 0.09727303683757782\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 9800 iterations: 1141.6316218852996 mins\n",
      "Train Loss: 0.08688917011022568\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10000 iterations: 1152.2492829163868 mins\n",
      "Train Loss: 0.11505214869976044\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10200 iterations: 1163.2798476338387 mins\n",
      "Train Loss: 0.09386833012104034\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10400 iterations: 1174.5871268153192 mins\n",
      "Train Loss: 0.10126479715108871\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10600 iterations: 1185.925987080733 mins\n",
      "Train Loss: 0.16182959079742432\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 10800 iterations: 1197.4163561662037 mins\n",
      "Train Loss: 0.08375443518161774\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11000 iterations: 1209.1893140157065 mins\n",
      "Train Loss: 0.10304540395736694\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11200 iterations: 1221.0101315339407 mins\n",
      "Train Loss: 0.11165212094783783\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11400 iterations: 1232.2584450006484 mins\n",
      "Train Loss: 0.09169726818799973\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11600 iterations: 1242.9166682839393 mins\n",
      "Train Loss: 0.11465467512607574\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 11800 iterations: 1253.5612720330557 mins\n",
      "Train Loss: 0.08199432492256165\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12000 iterations: 1263.8457001169522 mins\n",
      "Train Loss: 0.09178876131772995\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12200 iterations: 1274.1052562475204 mins\n",
      "Train Loss: 0.1281917840242386\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12400 iterations: 1284.4564136505128 mins\n",
      "Train Loss: 0.07843668758869171\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12600 iterations: 1294.874477350712 mins\n",
      "Train Loss: 0.08527343720197678\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 12800 iterations: 1305.686416165034 mins\n",
      "Train Loss: 0.10806016623973846\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13000 iterations: 1317.8156659007072 mins\n",
      "Train Loss: 0.20919102430343628\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13200 iterations: 1330.257539264361 mins\n",
      "Train Loss: 0.09395012259483337\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13400 iterations: 1342.3792163689932 mins\n",
      "Train Loss: 0.12772032618522644\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13600 iterations: 1354.3202698508899 mins\n",
      "Train Loss: 0.09154877066612244\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 13800 iterations: 1366.0502219319344 mins\n",
      "Train Loss: 0.08030427247285843\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14000 iterations: 1377.7867739637693 mins\n",
      "Train Loss: 0.07997041195631027\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14200 iterations: 1389.484102666378 mins\n",
      "Train Loss: 0.11481332778930664\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 95.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14400 iterations: 1401.049975947539 mins\n",
      "Train Loss: 0.11277616024017334\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14600 iterations: 1412.6517319520315 mins\n",
      "Train Loss: 0.08624792098999023\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 14800 iterations: 1423.6001481175422 mins\n",
      "Train Loss: 0.08857627213001251\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15000 iterations: 1434.5692909995714 mins\n",
      "Train Loss: 0.06458118557929993\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15200 iterations: 1444.7662368019421 mins\n",
      "Train Loss: 0.08805400878190994\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15400 iterations: 1454.7019351641336 mins\n",
      "Train Loss: 0.0652971938252449\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15600 iterations: 1464.5645015994708 mins\n",
      "Train Loss: 0.08224523812532425\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 15800 iterations: 1475.0391354004541 mins\n",
      "Train Loss: 0.0785931944847107\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16000 iterations: 1486.1629371841748 mins\n",
      "Train Loss: 0.06694547086954117\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16200 iterations: 1497.5274040818215 mins\n",
      "Train Loss: 0.07777367532253265\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16400 iterations: 1509.012689101696 mins\n",
      "Train Loss: 0.07118290662765503\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16600 iterations: 1520.49888758262 mins\n",
      "Train Loss: 0.16921517252922058\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 16800 iterations: 1532.0011485298476 mins\n",
      "Train Loss: 0.07274080067873001\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17000 iterations: 1542.4452101151148 mins\n",
      "Train Loss: 0.06402765214443207\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17200 iterations: 1552.4286126176517 mins\n",
      "Train Loss: 0.07333941757678986\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17400 iterations: 1562.3274035692216 mins\n",
      "Train Loss: 0.0827341377735138\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17600 iterations: 1572.0812083164851 mins\n",
      "Train Loss: 0.08614198863506317\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 17800 iterations: 1581.8113428314527 mins\n",
      "Train Loss: 0.06815089285373688\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 99.2% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18000 iterations: 1591.6188947677613 mins\n",
      "Train Loss: 0.09027071297168732\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18200 iterations: 1601.520783730348 mins\n",
      "Train Loss: 0.051946744322776794\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 6 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 99.6\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18400 iterations: 1611.331284181277 mins\n",
      "Train Loss: 0.09003917127847672\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18600 iterations: 1621.3215719501177 mins\n",
      "Train Loss: 0.1569606065750122\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.4% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 18800 iterations: 1632.1162629008293 mins\n",
      "Train Loss: 0.08070723712444305\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19000 iterations: 1643.3680047988892 mins\n",
      "Train Loss: 0.08297735452651978\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19200 iterations: 1654.8232520341874 mins\n",
      "Train Loss: 0.06377159059047699\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19400 iterations: 1667.1893727660179 mins\n",
      "Train Loss: 0.06335307657718658\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 97.6% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19600 iterations: 1679.2327778021495 mins\n",
      "Train Loss: 0.07263173907995224\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.0% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 19800 iterations: 1691.6608092983563 mins\n",
      "Train Loss: 0.14438550174236298\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Time for 20000 iterations: 1703.8455007513364 mins\n",
      "Train Loss: 0.062360528856515884\n",
      "Evaluating model on 250 random 6 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 96.8% 6 way one-shot learning accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs,targets) = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 6, 40, 40)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
